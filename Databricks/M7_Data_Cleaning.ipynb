{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "70316f83-c00a-4f2c-9552-832052d928f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Variables\n",
    "MOUNT_NAME = \"/mnt/user-bucket\"\n",
    "\n",
    "path_pin = f\"{MOUNT_NAME}/topics/0affe460a4c9.pin/partition=0/\"\n",
    "path_geo = f\"{MOUNT_NAME}/topics/0affe460a4c9.geo/partition=0/\"\n",
    "path_user = f\"{MOUNT_NAME}/topics/0affe460a4c9.user/partition=0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ee971b0d-306c-4b34-8bd5-241d9533b0a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load S3 Bucket Data into Spark DataFrames\n",
    "try:\n",
    "    df_pin = spark.read.format(\"json\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(path_pin)\n",
    "    \n",
    "    df_geo = spark.read.format(\"json\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(path_geo)\n",
    "    \n",
    "    df_user = spark.read.format(\"json\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(path_user)\n",
    "\n",
    "    # Display loaded data to verify successful loading (Data is Pre Cleaning)\n",
    "    print(\"Pinterest Data:\")\n",
    "    display(df_pin)\n",
    "\n",
    "    print(\"Geolocation Data:\")\n",
    "    display(df_geo)\n",
    "\n",
    "    print(\"User Data:\")\n",
    "    display(df_user)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from S3: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb4e1152-7d51-4ba9-95da-06e9c3d829d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import neccessary packages etc.\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, when, regexp_replace, concat_ws, to_timestamp, array, lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e69da3d0-f7a9-4acd-bf5f-6a3527910e49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Define Cleaning Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0a286b96-b9bc-402e-b88c-b6ab326d57ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class PinterestDataCleaner:\n",
    "    \"\"\"\n",
    "    A class to clean and preprocess Pinterest DataFrames.\n",
    "    \n",
    "    Attributes:\n",
    "        df (DataFrame): The Pinterest DataFrame to be cleaned.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: DataFrame):\n",
    "        \"\"\"\n",
    "        Initializes the PinterestDataCleaner with a DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "            df (DataFrame): The Pinterest DataFrame to be cleaned.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "    def replace_empty_with_none(self):\n",
    "        \"\"\"\n",
    "        Replaces empty entries and entries with no relevant data in each column with None.\n",
    "        \n",
    "        Returns:\n",
    "            self (PinterestDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        for column in self.df.columns:\n",
    "            self.df = self.df.withColumn(column, when(col(column) == \"\", None).otherwise(col(column)))\n",
    "        return self\n",
    "\n",
    "    def clean_follower_count(self):\n",
    "        \"\"\"\n",
    "        Cleans the follower_count column by converting entries with 'k' to thousands and 'M' to millions.\n",
    "        Ensures that the data type is an integer.\n",
    "        \n",
    "        Returns:\n",
    "            self (PinterestDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        self.df = self.df.withColumn(\n",
    "            \"follower_count\",\n",
    "            when(col(\"follower_count\").rlike(r'^[\\d]+k$'), (regexp_replace(col(\"follower_count\"), \"k\", \"\").cast(\"int\") * 1000))\n",
    "            .when(col(\"follower_count\").rlike(r'^[\\d]+M$'), (regexp_replace(col(\"follower_count\"), \"M\", \"\").cast(\"int\") * 1000000))\n",
    "            .otherwise(col(\"follower_count\").cast(\"int\"))\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def convert_columns_to_int(self, columns):\n",
    "        \"\"\"\n",
    "        Converts specified columns to the int data type after formatting.\n",
    "        \n",
    "        Parameters:\n",
    "            columns (list): List of columns to convert to int data type.\n",
    "        \n",
    "        Returns:\n",
    "            self (PinterestDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        for column in columns:\n",
    "            self.df = self.df.withColumn(column, col(column).cast(\"int\"))\n",
    "        return self\n",
    "\n",
    "    def convert_corrupt_record_to_boolean(self):\n",
    "        \"\"\"\n",
    "        Converts the '_corrupt_record' column to a boolean.\n",
    "        Sets 'null' values to False and all other values to True.\n",
    "        \n",
    "        Returns:\n",
    "            self (PinterestDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        self.df = self.df.withColumn(\n",
    "            \"_corrupt_record\",\n",
    "            when(col(\"_corrupt_record\").isNull(), lit(False)).otherwise(lit(True))\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def clean_save_location(self):\n",
    "        \"\"\"\n",
    "        Cleans the save_location column to include only the save location path.\n",
    "        \n",
    "        Returns:\n",
    "            self (PinterestDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        self.df = self.df.withColumn(\n",
    "            \"save_location\",\n",
    "            regexp_replace(col(\"save_location\"), r\"https?://[^/]+/\", \"\")\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def rename_index_column(self, old_name: str, new_name: str):\n",
    "        \"\"\"\n",
    "        Renames a specified index column.\n",
    "        \n",
    "        Parameters:\n",
    "            old_name (str): The current name of the index column.\n",
    "            new_name (str): The new name for the index column.\n",
    "        \n",
    "        Returns:\n",
    "            self (PinterestDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        self.df = self.df.withColumnRenamed(old_name, new_name)\n",
    "        return self\n",
    "    \n",
    "    def drop_rows_where_corrupt(self):\n",
    "        \"\"\"\n",
    "        Filters out rows where '_corrupt_record' is True.\n",
    "        \n",
    "        Returns:\n",
    "            self (UserDataCleaner): Returns the instance itself to allow method chaining.\n",
    "            This method drops rows in the DataFrame where the '_corrupt_record' column is True.\n",
    "        \"\"\"\n",
    "        # Filter to show rows where _corrupt_record is False\n",
    "        self.df = self.df.filter(self.df._corrupt_record == False)\n",
    "        return self\n",
    "\n",
    "    def reorder_columns(self, column_order: list):\n",
    "        \"\"\"\n",
    "        Reorders the DataFrame columns to a specified order.\n",
    "        \n",
    "        Parameters:\n",
    "            column_order (list): A list specifying the desired column order.\n",
    "        \n",
    "        Returns:\n",
    "            self (PinterestDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        self.df = self.df.select(column_order)\n",
    "        return self\n",
    "\n",
    "    def clean_pin_df(self):\n",
    "        \"\"\"\n",
    "        Cleans the Pinterest DataFrame by performing the necessary transformations.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: The cleaned Pinterest DataFrame.\n",
    "        \"\"\"\n",
    "        columns_to_convert = [\"index\", \"follower_count\", \"downloaded\"]\n",
    "\n",
    "        self.replace_empty_with_none()\\\n",
    "            .clean_follower_count()\\\n",
    "            .convert_corrupt_record_to_boolean()\\\n",
    "            .clean_save_location()\\\n",
    "            .convert_columns_to_int(columns_to_convert)\\\n",
    "            .rename_index_column(old_name=\"index\", new_name=\"ind\")\\\n",
    "            .drop_rows_where_corrupt()\\\n",
    "            .reorder_columns([\n",
    "                \"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\",\n",
    "                \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\",\n",
    "                \"save_location\", \"category\", \"downloaded\", \"_corrupt_record\"\n",
    "            ])\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f49f9766-3528-4144-bab1-772c064a165d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class GeoDataCleaner:\n",
    "    \"\"\"\n",
    "    A class to clean and preprocess Geo DataFrames.\n",
    "    \n",
    "    Attributes:\n",
    "        df (DataFrame): The Geo DataFrame to be cleaned.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: DataFrame):\n",
    "        \"\"\"\n",
    "        Initializes the GeoDataCleaner with a DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "            df (DataFrame): The Geo DataFrame to be cleaned.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "    def create_coordinates(self):\n",
    "        \"\"\"\n",
    "        Creates a new column 'coordinates' from the 'latitude' and 'longitude' columns,\n",
    "        with named fields 'longitude' and 'latitude'.\n",
    "        \n",
    "        Returns:\n",
    "            self (GeoDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        self.df = self.df.withColumn(\"coordinates\", struct(\n",
    "            col(\"longitude\").alias(\"longitude\"),\n",
    "            col(\"latitude\").alias(\"latitude\")\n",
    "        ))\n",
    "        return self\n",
    "\n",
    "    def drop_columns(self, columns):\n",
    "        \"\"\"\n",
    "        Drops specified columns from the DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "            columns (list): A list of column names to be dropped.\n",
    "        \n",
    "        Returns:\n",
    "            self (GeoDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        self.df = self.df.drop(*columns)\n",
    "        return self\n",
    "\n",
    "    def convert_timestamp(self, column_name: str):\n",
    "        \"\"\"\n",
    "        Converts a string column to a timestamp data type.\n",
    "        \n",
    "        Parameters:\n",
    "            column_name (str): The name of the column to be converted.\n",
    "        \n",
    "        Returns:\n",
    "            self (GeoDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        self.df = self.df.withColumn(column_name, to_timestamp(col(column_name)))\n",
    "        return self\n",
    "\n",
    "    def reorder_columns(self, column_order: list):\n",
    "        \"\"\"\n",
    "        Reorders the DataFrame columns to a specified order.\n",
    "        \n",
    "        Parameters:\n",
    "            column_order (list): A list specifying the desired column order.\n",
    "        \n",
    "        Returns:\n",
    "            self (GeoDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        self.df = self.df.select(column_order)\n",
    "        return self\n",
    "\n",
    "    def clean_geo_df(self):\n",
    "        \"\"\"\n",
    "        Cleans the Geo DataFrame by performing the necessary transformations.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: The cleaned Geo DataFrame.\n",
    "        \"\"\"\n",
    "        self.create_coordinates()\\\n",
    "            .drop_columns([\"latitude\", \"longitude\"])\\\n",
    "            .convert_timestamp(\"timestamp\")\\\n",
    "            .reorder_columns([\"ind\", \"country\", \"coordinates\", \"timestamp\"])\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8adf10dc-bdaa-40db-a730-c694e44bc707",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class UserDataCleaner:\n",
    "    \"\"\"\n",
    "    A class to clean and preprocess User DataFrames.\n",
    "    \n",
    "    Attributes:\n",
    "        df (DataFrame): The User DataFrame to be cleaned.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: DataFrame):\n",
    "        \"\"\"\n",
    "        Initializes the UserDataCleaner with a DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "            df (DataFrame): The User DataFrame to be cleaned.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "\n",
    "    def create_user_name(self):\n",
    "        \"\"\"\n",
    "        Creates a new column 'user_name' by concatenating 'first_name' and 'last_name'.\n",
    "        \n",
    "        Returns:\n",
    "            self (UserDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        self.df = self.df.withColumn(\"user_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "        return self\n",
    "\n",
    "    def drop_columns(self, columns):\n",
    "        \"\"\"\n",
    "        Drops specified columns from the DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "            columns (list): A list of column names to be dropped.\n",
    "        \n",
    "        Returns:\n",
    "            self (UserDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        self.df = self.df.drop(*columns)\n",
    "        return self\n",
    "\n",
    "    def convert_timestamp(self, column_name: str):\n",
    "        \"\"\"\n",
    "        Converts a string column to a timestamp data type.\n",
    "        \n",
    "        Parameters:\n",
    "            column_name (str): The name of the column to be converted.\n",
    "        \n",
    "        Returns:\n",
    "            self (UserDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        self.df = self.df.withColumn(column_name, to_timestamp(col(column_name)))\n",
    "        return self\n",
    "\n",
    "    def convert_columns_to_int(self, columns):\n",
    "        \"\"\"\n",
    "        Converts specified columns to the int data type.\n",
    "        \n",
    "        Parameters:\n",
    "            columns (list): List of columns to convert to int data type.\n",
    "        \n",
    "        Returns:\n",
    "            self (UserDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        for column in columns:\n",
    "            self.df = self.df.withColumn(column, col(column).cast(\"int\"))\n",
    "        return self\n",
    "\n",
    "    def reorder_columns(self, column_order: list):\n",
    "        \"\"\"\n",
    "        Reorders the DataFrame columns to a specified order.\n",
    "        \n",
    "        Parameters:\n",
    "            column_order (list): A list specifying the desired column order.\n",
    "        \n",
    "        Returns:\n",
    "            self (UserDataCleaner): Returns the instance itself to allow method chaining.\n",
    "        \"\"\"\n",
    "        self.df = self.df.select(column_order)\n",
    "        return self\n",
    "\n",
    "    def clean_user_df(self):\n",
    "        \"\"\"\n",
    "        Cleans the User DataFrame by performing the necessary transformations.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: The cleaned User DataFrame.\n",
    "        \"\"\"\n",
    "        int_columns = [\"ind\", \"age\"]\n",
    "\n",
    "        self.create_user_name()\\\n",
    "            .drop_columns([\"first_name\", \"last_name\"])\\\n",
    "            .convert_timestamp(\"date_joined\")\\\n",
    "            .convert_columns_to_int(int_columns)\\\n",
    "            .reorder_columns([\"ind\", \"user_name\", \"age\", \"date_joined\"])\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c087fcf5-2a5d-4c9b-8096-a28d4e7e6f9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Initialize the cleaners & run the cleaning scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5289e231-e842-4feb-9f7d-26db35bdf592",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the cleaners with the proper DataFrames\n",
    "pin_cleaner = PinterestDataCleaner(df_pin)\n",
    "geo_cleaner = GeoDataCleaner(df_geo)\n",
    "user_cleaner = UserDataCleaner(df_user)\n",
    "\n",
    "# Clean the DataFrames\n",
    "df_pin_cleaned = pin_cleaner.clean_pin_df()\n",
    "df_geo_cleaned = geo_cleaner.clean_geo_df()\n",
    "df_user_cleaned = user_cleaner.clean_user_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2b11d80d-e2fb-477c-980f-d8f1c548d663",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display Cleaned Pinterest DataFrame\n",
    "display(df_pin_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1b0179cb-9e72-431c-a46d-3ee30add6d95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display Cleaned Geo DataFrame\n",
    "display(df_geo_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e4aac424-e739-4afd-bf95-fdb6323c036b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display Cleaned User DataFrame\n",
    "display(df_user_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7333049-0238-4e9d-b59f-0d1b43659aff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Save DataFrames as new Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c48661d4-97b2-4929-9a72-1f1427acf57d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create global temporary views\n",
    "df_pin_cleaned.createOrReplaceGlobalTempView(\"global_cleaned_pin\")\n",
    "df_geo_cleaned.createOrReplaceGlobalTempView(\"global_cleaned_geo\")\n",
    "df_user_cleaned.createOrReplaceGlobalTempView(\"global_cleaned_user\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1259537191187465,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "M7_Data_Cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
