{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2955f1c7-f5b6-47ba-9eb6-5e644a9b24ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Important Notes:\n",
    "- **Ensure that our streams are correctly configured and that data is being sent to these streams before running these commands.**\n",
    "> This means our `user_posting_emulation_streaming.py` file should be running!\n",
    "- **The display function will continuously update the Databricks notebook as new data arrives in the stream.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c49da1-ca5a-451b-aa74-cddc12fbe108",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Step 1: Load AWS Credentials\n",
    "Ensure that we load our AWS credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9969866-08ab-4491-8d1e-d66d1e358ce4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table containing AWS credentials\n",
    "delta_table_path = \"<your_delta_table_path>\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Extract AWS Access Key and Secret Key\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dd7163e-6f4b-45ed-8429-25cef05273fe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Step 2: Read Data from Each Stream\n",
    "We will repeat the reading process for each of the three streams (PIN_STREAM, GEO_STREAM, and USER_STREAM from our `user_posting_emulation_streaming.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0342c9fe-1e67-4f74-bba5-b63f4bce130c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "/*Disable format checks during the reading of Delta tables*/\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "00c6d531-50b5-48fb-90b1-99d0cbc2c19a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read data from the Pinterest stream\n",
    "pin_df = spark \\\n",
    "  .readStream \\\n",
    "  .format('kinesis') \\\n",
    "  .option('streamName', 'streaming-<your_user_ID>-pin') \\\n",
    "  .option('initialPosition', 'earliest') \\\n",
    "  .option('region', 'us-east-1') \\\n",
    "  .option('awsAccessKey', ACCESS_KEY) \\\n",
    "  .option('awsSecretKey', SECRET_KEY) \\\n",
    "  .load()\n",
    "\n",
    "# Read data from the Geolocation stream\n",
    "geo_df = spark \\\n",
    "  .readStream \\\n",
    "  .format('kinesis') \\\n",
    "  .option('streamName', 'streaming-<your_user_ID>-geo') \\\n",
    "  .option('initialPosition', 'earliest') \\\n",
    "  .option('region', 'us-east-1') \\\n",
    "  .option('awsAccessKey', ACCESS_KEY) \\\n",
    "  .option('awsSecretKey', SECRET_KEY) \\\n",
    "  .load()\n",
    "\n",
    "# Read data from the User stream\n",
    "user_df = spark \\\n",
    "  .readStream \\\n",
    "  .format('kinesis') \\\n",
    "  .option('streamName', 'streaming-<your_user_ID>-user') \\\n",
    "  .option('initialPosition', 'earliest') \\\n",
    "  .option('region', 'us-east-1') \\\n",
    "  .option('awsAccessKey', ACCESS_KEY) \\\n",
    "  .option('awsSecretKey', SECRET_KEY) \\\n",
    "  .load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ec6f942-5e0b-49fc-a2fa-b0721dad646e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Step 3: Display the Streaming Data\n",
    "We can now display the streaming data for each stream using the display function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "54df8d2f-6018-49a7-89df-da76abd08fe9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the contents of pin_df\n",
    "display(pin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "79e08c44-9090-4743-aa22-87a26ddc6a6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the contents of geo_df\n",
    "display(geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c939bde7-5828-484f-adef-d25ebad2e60e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the contents of user_df\n",
    "display(user_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8451e1e8-d5d7-45ed-83f3-1cb1e05c262a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Step 4: Deserialize the Data Columns\n",
    "If we want to view the actual data contained in the stream, we can cast the data column to a string for each DataFrame, then view the deserialized data to better understand what needs to be done to clean it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9efb669-fe47-42f5-a20a-261b7f359f54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cast_pin_df = pin_df.selectExpr(\"CAST(data AS STRING) as jsonData\")\n",
    "cast_geo_df = geo_df.selectExpr(\"CAST(data AS STRING) as jsonData\")\n",
    "cast_user_df = user_df.selectExpr(\"CAST(data AS STRING) as jsonData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0ab47561-7108-45cb-8f55-9ea5e923b773",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cast_pin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7d3535f8-6a49-4fa0-9372-a797fb190e51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cast_geo_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c53e96a9-5862-4a9f-b02b-d447673ec5c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cast_user_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78ad4865-241d-43b8-9614-29f93ac06095",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Step 5: Parse the JSON Strings\n",
    "The data is currently stored as JSON strings within a single column. We need to parse these JSON strings to convert them into separate columns for easier manipulation.\n",
    "\n",
    "Here's how to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acdf73ec-3562-4fe8-9483-e139dd7f8949",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "\n",
    "# Define the schema for each stream based on the expected structure of the JSON data\n",
    "geo_schema = StructType([\n",
    "    StructField(\"ind\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"latitude\", StringType(), True),\n",
    "    StructField(\"longitude\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "pin_schema = StructType([\n",
    "    StructField(\"index\", StringType(), True),\n",
    "    StructField(\"unique_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"downloaded\", StringType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "user_schema = StructType([\n",
    "    StructField(\"ind\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"date_joined\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d736e2fe-0705-4305-a9b3-617389043aa1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parse the JSON data based on the stream you are checking\n",
    "parsed_geo_df = cast_geo_df.select(from_json(\"jsonData\", geo_schema).alias(\"geo_data\")).select(\"geo_data.*\")\n",
    "parsed_pin_df = cast_pin_df.select(from_json(\"jsonData\", pin_schema).alias(\"pin_data\")).select(\"pin_data.*\")\n",
    "parsed_user_df = cast_user_df.select(from_json(\"jsonData\", user_schema).alias(\"user_data\")).select(\"user_data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0458efce-c456-4112-a86a-1a9d54161a06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Display the parsed dataframes to better understand what transformations need to be made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "116c745a-690f-4a11-9b9c-b48e6bf563b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(parsed_pin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "67e38713-88de-42ca-aa5c-711f69e45249",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(parsed_geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8afc071d-53bc-4458-8b16-9604db74eb15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(parsed_user_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96fb0119-2795-4e06-8bf9-503abb5b6486",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Step 6: Transform the Data\n",
    "Once we've parsed the JSON strings, we need to clean the data. We can copy most of our existing cleaning classes used for the batch processing steps in an prior milestone.\n",
    "\n",
    "Check to see if any adjustments to cleaning classes are neccessary (I had to remove some methods that were focused on the `_corrupt_record` column that no longer exists as a result of changing my way of sending data to kinesis and reading into databricks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1f45e13d-e474-4c72-aadc-290e6c5251fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class PinterestDataCleaner:\n",
    "    def __init__(self, df: DataFrame):\n",
    "        self.df = df\n",
    "\n",
    "    def replace_empty_with_none(self):\n",
    "        for column in self.df.columns:\n",
    "            self.df = self.df.withColumn(column, when(col(column) == \"\", None).otherwise(col(column)))\n",
    "        return self\n",
    "\n",
    "    def clean_follower_count(self):\n",
    "        self.df = self.df.withColumn(\n",
    "            \"follower_count\",\n",
    "            when(col(\"follower_count\").rlike(r'^[\\d]+k$'), (regexp_replace(col(\"follower_count\"), \"k\", \"\").cast(\"int\") * 1000))\n",
    "            .when(col(\"follower_count\").rlike(r'^[\\d]+M$'), (regexp_replace(col(\"follower_count\"), \"M\", \"\").cast(\"int\") * 1000000))\n",
    "            .otherwise(col(\"follower_count\").cast(\"int\"))\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def convert_columns_to_int(self, columns):\n",
    "        for column in columns:\n",
    "            self.df = self.df.withColumn(column, col(column).cast(\"int\"))\n",
    "        return self\n",
    "\n",
    "#    def convert_corrupt_record_to_boolean(self):\n",
    "#        self.df = self.df.withColumn(\n",
    "#            \"_corrupt_record\",\n",
    "#            when(col(\"_corrupt_record\").isNull(), lit(False)).otherwise(lit(True)).cast(\"boolean\")\n",
    "#        )\n",
    "#        return self\n",
    "\n",
    "    def clean_save_location(self):\n",
    "        self.df = self.df.withColumn(\n",
    "            \"save_location\",\n",
    "            regexp_replace(col(\"save_location\"), r\"https?://[^/]+/\", \"\").cast(\"string\")\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def rename_index_column(self, old_name: str, new_name: str):\n",
    "        self.df = self.df.withColumnRenamed(old_name, new_name)\n",
    "        return self\n",
    "    \n",
    "    #def drop_rows_where_corrupt(self):\n",
    "    #    self.df = self.df.filter(self.df._corrupt_record == False)\n",
    "    #    return self\n",
    "\n",
    "    def reorder_columns(self, column_order: list):\n",
    "        self.df = self.df.select(column_order)\n",
    "        return self\n",
    "\n",
    "    def clean_pin_df(self):\n",
    "        columns_to_convert = [\"index\", \"follower_count\", \"downloaded\"]\n",
    "\n",
    "        self.replace_empty_with_none()\\\n",
    "            .clean_follower_count()\\\n",
    "            .clean_save_location()\\\n",
    "            .convert_columns_to_int(columns_to_convert)\\\n",
    "            .rename_index_column(old_name=\"index\", new_name=\"ind\")\\\n",
    "            .reorder_columns([\n",
    "                \"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\",\n",
    "                \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\",\n",
    "                \"save_location\", \"category\", \"downloaded\"#, \"_corrupt_record\"\n",
    "            ])\n",
    "        return self.df\n",
    "    \n",
    "            # removed these from the clean_pin_df method as they were no longer present in the dataframe when loaded into databricks\n",
    "            #.drop_rows_where_corrupt()\\\n",
    "            #.convert_corrupt_record_to_boolean()\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f4ea4b80-9076-4ef3-b33a-ac770baa49c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class GeoDataCleaner:\n",
    "    def __init__(self, df: DataFrame):\n",
    "        self.df = df\n",
    "\n",
    "    def create_coordinates(self):\n",
    "        self.df = self.df.withColumn(\"coordinates\", struct(\n",
    "            col(\"longitude\").cast(\"double\").alias(\"longitude\"),\n",
    "            col(\"latitude\").cast(\"double\").alias(\"latitude\")\n",
    "        ))\n",
    "        return self\n",
    "\n",
    "    def drop_columns(self, columns):\n",
    "        self.df = self.df.drop(*columns)\n",
    "        return self\n",
    "\n",
    "    def convert_timestamp(self, column_name: str):\n",
    "        self.df = self.df.withColumn(column_name, to_timestamp(col(column_name)))\n",
    "        return self\n",
    "\n",
    "    def reorder_columns(self, column_order: list):\n",
    "        self.df = self.df.select(column_order)\n",
    "        return self\n",
    "\n",
    "    def clean_geo_df(self):\n",
    "        self.create_coordinates()\\\n",
    "            .drop_columns([\"latitude\", \"longitude\"])\\\n",
    "            .convert_timestamp(\"timestamp\")\\\n",
    "            .reorder_columns([\"ind\", \"country\", \"coordinates\", \"timestamp\"])\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "98c5d3ed-5ff8-4363-908c-425966e8ac17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class UserDataCleaner:\n",
    "    def __init__(self, df: DataFrame):\n",
    "        self.df = df\n",
    "\n",
    "    def create_user_name(self):\n",
    "        self.df = self.df.withColumn(\"user_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")).cast(\"string\"))\n",
    "        return self\n",
    "\n",
    "    def drop_columns(self, columns):\n",
    "        self.df = self.df.drop(*columns)\n",
    "        return self\n",
    "\n",
    "    def convert_timestamp(self, column_name: str):\n",
    "        self.df = self.df.withColumn(column_name, to_timestamp(col(column_name)))\n",
    "        return self\n",
    "\n",
    "    def convert_columns_to_int(self, columns):\n",
    "        for column in columns:\n",
    "            self.df = self.df.withColumn(column, col(column).cast(\"int\"))\n",
    "        return self\n",
    "\n",
    "    def reorder_columns(self, column_order: list):\n",
    "        self.df = self.df.select(column_order)\n",
    "        return self\n",
    "\n",
    "    def clean_user_df(self):\n",
    "        int_columns = [\"ind\", \"age\"]\n",
    "\n",
    "        self.create_user_name()\\\n",
    "            .drop_columns([\"first_name\", \"last_name\"])\\\n",
    "            .convert_timestamp(\"date_joined\")\\\n",
    "            .convert_columns_to_int(int_columns)\\\n",
    "            .reorder_columns([\"ind\", \"user_name\", \"age\", \"date_joined\"])\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c423b70c-5f28-4254-bb70-5f691a675c92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the cleaners with the parsed DataFrames\n",
    "pin_cleaner = PinterestDataCleaner(parsed_pin_df)\n",
    "geo_cleaner = GeoDataCleaner(parsed_geo_df)\n",
    "user_cleaner = UserDataCleaner(parsed_user_df)\n",
    "\n",
    "# Clean the DataFrames\n",
    "df_pin_cleaned = pin_cleaner.clean_pin_df()\n",
    "df_geo_cleaned = geo_cleaner.clean_geo_df()\n",
    "df_user_cleaned = user_cleaner.clean_user_df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad482122-2f71-4401-8cbd-2242ecb87ca0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Check the cleaned dataframes to see if you're happy with the transformations that have been made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cb41fc1c-5922-418c-a2aa-fbd872548ef4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_pin_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "79d5081e-7e2f-44c1-9380-f0383e1692c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_geo_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b2afa7da-c866-42b0-899c-105c66851ae7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_user_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "134c0eed-9bae-4d7b-9265-239c78b7d30c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Step 7: Write the Streaming Data to Delta Tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "767618bd-388c-4e5f-a440-39ea39143b15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For df_pin_cleaned writing to <your_user_ID>_pin_table\n",
    "df_pin_cleaned.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/mnt/delta/checkpoints/<your_user_ID>_pin_table/\") \\\n",
    "  .toTable(\"<your_user_ID>_pin_table\")\n",
    "\n",
    "# For df_geo_cleaned writing to <your_user_ID>_geo_table\n",
    "df_geo_cleaned.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/mnt/delta/checkpoints/<your_user_ID>_geo_table/\") \\\n",
    "  .toTable(\"<your_user_ID>_geo_table\")\n",
    "\n",
    "# For df_user_cleaned writing to <your_user_ID>_user_table\n",
    "df_user_cleaned.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/mnt/delta/checkpoints/<your_user_ID>_user_table/\") \\\n",
    "  .toTable(\"<your_user_ID>_user_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cc3d5be-18b0-4368-971e-2696c0b2ac21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Step 8: Testing Delta Table 'writes'\n",
    "The below steps are completed to confirm the success of writing the streaming data to new Delta Tables & gather various forms of information to gather an idea of the current state of the cleaned streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "78778e23-f763-434b-8a3e-1c24efb715c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List the Contents of the Checkpoint Directory:\n",
    "%fs ls /mnt/delta/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b6b1c41e-d9a1-4f13-807d-638067df1d00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check the contents of the pin table\n",
    "SELECT * FROM <your_user_ID>_pin_table LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8cc2dc1b-dc83-4a67-9579-46f891764e4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check the contents of the geo table\n",
    "SELECT * FROM <your_user_ID>_geo_table LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "65702f84-13bc-4823-ae47-12be0d1c0216",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check the contents of the user table\n",
    "SELECT * FROM <your_user_ID>_user_table LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "793347f2-2515-4cfc-b564-d98b822bc572",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check the metadata of the pin table\n",
    "DESCRIBE DETAIL <your_user_ID>_pin_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8e3186b7-45e3-4e94-9f7b-54703b74990f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check the metadata of the geo table\n",
    "DESCRIBE DETAIL <your_user_ID>_geo_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d947f6d0-c512-4af6-88ac-6866393f6d35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check the metadata of the user table\n",
    "DESCRIBE DETAIL <your_user_ID>_user_table;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "adc38bad-9489-4338-a74d-dafc6d7df038",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View the history of operations on the pin table\n",
    "DESCRIBE HISTORY <your_user_ID>_pin_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "17da1db9-51e8-4ddd-8911-e3f5e4562f4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View the history of operations on the geo table\n",
    "DESCRIBE HISTORY <your_user_ID>_geo_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ff65ef55-8726-4342-9c4a-d0e5811cea08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View the history of operations on the user table\n",
    "DESCRIBE HISTORY <your_user_ID>_user_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e6488cb3-8ac2-4f26-adf1-2552d4019048",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Count rows in the pin table\n",
    "SELECT COUNT(*) FROM <your_user_ID>_pin_table;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "595d2747-6490-4831-8b7b-f788ad70b510",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Count rows in the geo table\n",
    "SELECT COUNT(*) FROM <your_user_ID>_geo_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1844ae2a-ca7e-4e49-9290-c063d5e99f9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Count rows in the user table\n",
    "SELECT COUNT(*) FROM <your_user_ID>_user_table;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1386878133729227,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "working_kin_read_write",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
