{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**Please Note: This code is meant to be run within a DataBricks notebook.**\n",
    "\n",
    "Securely mounts an S3 bucket to Databricks and loads data into DataFrames.\n",
    "\n",
    "1. Load AWS credentials from a Delta table.\n",
    "2. Encode the secret key for security.\n",
    "3. Construct the S3 source URL.\n",
    "4. Mount the S3 bucket to the Databricks File System (DBFS).\n",
    "5. Load JSON data into DataFrames.\n",
    "\n",
    "DataFrames:\n",
    "- df_pin: Pinterest post data.\n",
    "- df_geo: Geolocation data.\n",
    "- df_user: User data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql.functions import *\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load AWS credentials from Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract and encode credentials\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the S3 bucket name and mount point\n",
    "AWS_S3_BUCKET = \"user-0affe460a4c9-bucket\"  # Replace with your S3 bucket name\n",
    "MOUNT_NAME = \"/mnt/user-bucket\"  # Choose a meaningful mount name\n",
    "SOURCE_URL = f\"s3n://{ACCESS_KEY}:{ENCODED_SECRET_KEY}@{AWS_S3_BUCKET}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Mount the S3 bucket\n",
    "try:\n",
    "    dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)\n",
    "    print(f\"Successfully mounted {AWS_S3_BUCKET} to {MOUNT_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting {AWS_S3_BUCKET}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define paths to the JSON files in the S3 bucket\n",
    "# Replace <your_UserId> with your actual user ID or the appropriate path in your S3 bucket\n",
    "path_pin = f\"{MOUNT_NAME}/topics/0affe460a4c9.pin/partition=0/\"\n",
    "path_geo = f\"{MOUNT_NAME}/topics/0affe460a4c9.geo/partition=0/\"\n",
    "path_user = f\"{MOUNT_NAME}/topics/0affe460a4c9.user/partition=0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "/*Step 6: Disable format checks during the reading of Delta tables*/\n",
    "SET spark.databricks.delta.formatCheck.enabled=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Load the JSON data into DataFrames with schema inference\n",
    "try:\n",
    "    df_pin = spark.read.format(\"json\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(path_pin)\n",
    "    \n",
    "    df_geo = spark.read.format(\"json\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(path_geo)\n",
    "    \n",
    "    df_user = spark.read.format(\"json\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(path_user)\n",
    "\n",
    "    # Display loaded data to verify successful loading\n",
    "    print(\"Pinterest Data:\")\n",
    "    display(df_pin)\n",
    "\n",
    "    print(\"Geolocation Data:\")\n",
    "    display(df_geo)\n",
    "\n",
    "    print(\"User Data:\")\n",
    "    display(df_user)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from S3: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
